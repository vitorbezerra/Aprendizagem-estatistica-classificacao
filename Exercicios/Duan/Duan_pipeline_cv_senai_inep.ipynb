{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercicio\n",
    "\n",
    "#### Montem uma classificacao utilizando a base da Veltec ou Senai\n",
    "#### Monte testes de avaliacao de diferentes classificadores considerando:\n",
    "* Busca por hiperparametros \n",
    "    * Considere testar parametros de regularizacao\n",
    "* Busca por features\n",
    "* Utilize um metodo de validacao cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulacao de dados e operacoes\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from numpy import median\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# Visualizacao\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Estatistica\n",
    "from statistics import mean \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import classification_report    # output: accuracy, f1-score, recall e precision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Modelos de regressao\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Modelos de Classificação\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Outras bibliotecas de modelos\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, LeaveOneOut\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Pre-processamento\n",
    "from sklearn.preprocessing import scale, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Outros\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.special import expit\n",
    "from scipy.io import arff\n",
    "from sklearn.datasets import fetch_olivetti_faces, load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo a base de dados\n",
    "senai = pd.read_csv(\"senai_inep2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3309, 22)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho do dataframe\n",
    "senai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#duplicados = senai[senai['ID_ALUNO'].duplicated() == True]['ID_ALUNO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando as bases em X e y\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificando colunas categóricas\n",
    "le = LabelEncoder()\n",
    "senai['NO_IES'] = le.fit_transform(senai['NO_IES'])\n",
    "senai['NO_CURSO'] = le.fit_transform(senai['NO_IES'])\n",
    "senai['ID_ALUNO'] = le.fit_transform(senai['ID_ALUNO'])\n",
    "senai['DT_INGRESSO_CURSO'] = le.fit_transform(senai['DT_INGRESSO_CURSO'])\n",
    "\n",
    "# Saída a ser predita \"y\"\n",
    "X = senai.drop(['TP_SITUACAO'], axis=1)\n",
    "y = senai[['TP_SITUACAO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando scores dos modelos:\n",
    "\n",
    "*Decision Tree* apresenta maior score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score: 0.8873239436619719\n",
      "KNN score: 0.8812877263581489\n",
      "Tree score: 0.8853118712273642\n",
      "Naive Bayes score: 0.8873239436619719\n",
      "Ridge score: 0.9114688128772636\n",
      "SGD score: 0.8873239436619719\n"
     ]
    }
   ],
   "source": [
    "# Regressao Logistica\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train.values.ravel())\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression score:\", log_reg.score(X_test, y_test))\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train.values.ravel())\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"KNN score:\", knn.score(X_test, y_test))\n",
    "\n",
    "# Arvore de Decisao\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Tree score:\", tree.score(X_test, y_test))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train.values.ravel())\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Naive Bayes score:\", nb.score(X_test, y_test))\n",
    "\n",
    "# Ridge\n",
    "ridge = RidgeClassifier()\n",
    "ridge.fit(X_train, y_train.values.ravel())\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(\"Ridge score:\", ridge.score(X_test, y_test))\n",
    "\n",
    "# SGD\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train.values.ravel())\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(\"SGD score:\", sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo a mesma coisa, mas agora com Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.909\n",
      "KNN pipeline test accuracy: 0.903\n",
      "Decision Tree pipeline test accuracy: 0.887\n",
      "Naive Bayes pipeline test accuracy: 0.821\n",
      "Ridge pipeline test accuracy: 0.911\n",
      "SGD pipeline test accuracy: 0.903\n",
      "---\n",
      "Classifier with best accuracy: Ridge\n"
     ]
    }
   ],
   "source": [
    "# Construindo pipelines\n",
    "pipe_lr    = Pipeline( [ ('scl', StandardScaler()), ('clf', LogisticRegression()) ] )\n",
    "pipe_knn   = Pipeline([('scl', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipe_dt    = Pipeline([('scl', StandardScaler()), ('clf', DecisionTreeClassifier())])\n",
    "pipe_nb    = Pipeline([('scl', StandardScaler()), ('clf', GaussianNB())])\n",
    "pipe_ridge = Pipeline([('scl', StandardScaler()), ('clf', RidgeClassifier())])\n",
    "pipe_sgd = Pipeline([('scl', StandardScaler()), ('clf', SGDClassifier())])\n",
    "\n",
    "\n",
    "# Lista de pipelines a serem executados\n",
    "pipelines = [pipe_lr, pipe_knn, pipe_dt, pipe_nb, pipe_ridge, pipe_sgd]\n",
    "\n",
    "# Dicionário para facilitar identificacao\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'KNN', 2: 'Decision Tree', 3: 'Naive Bayes', 4: 'Ridge', 5: 'SGD'}\n",
    "\n",
    "# aplicando fit\n",
    "# Generaliza a execucao do fit de cada ultima funcao do pipe\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Compara acurácia\n",
    "for i, pipe in enumerate(pipelines):\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipe_dict[i], pipe.score(X_test, y_test)))\n",
    "\n",
    "# para cada modelo treinado obtem val score\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for i, pipe in enumerate(pipelines):\n",
    "    # Descobre o melhor val.score e armazen em best_clf\n",
    "    if pipe.score(X_test, y_test) > best_acc:\n",
    "        best_acc = pipe.score(X_test, y_test)\n",
    "        best_pipe = pipe\n",
    "        best_clf = i\n",
    "print('---\\nClassifier with best accuracy: %s' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_pipeline_senai.pkl']"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvando arquivo com o melhor modelo encontrado acima\n",
    "joblib.dump(best_pipe, 'best_pipeline_senai.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busca por hiperparametros usando GridSearchCV\n",
    "---\n",
    "A seguir será buscada, dentre todas as combinações definidas de hiperparametros, a combinação com o melhor score (e irá retornar esse valor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanc\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\duanc\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\duanc\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\duanc\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 100}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.799 (+/-0.101) for {'alpha': 1}\n",
      "0.789 (+/-0.111) for {'alpha': 10}\n",
      "0.921 (+/-0.066) for {'alpha': 100}\n",
      "0.434 (+/-0.001) for {'alpha': 1000}\n",
      "\n",
      "classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.07      0.13        56\n",
      "           1       0.89      1.00      0.94       441\n",
      "\n",
      "    accuracy                           0.89       497\n",
      "   macro avg       0.85      0.53      0.54       497\n",
      "weighted avg       0.88      0.89      0.85       497\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.615 (+/-0.050) for {'alpha': 1}\n",
      "0.591 (+/-0.041) for {'alpha': 10}\n",
      "0.528 (+/-0.026) for {'alpha': 100}\n",
      "0.500 (+/-0.001) for {'alpha': 1000}\n",
      "\n",
      "classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.29      0.42        56\n",
      "           1       0.92      0.99      0.95       441\n",
      "\n",
      "    accuracy                           0.91       497\n",
      "   macro avg       0.86      0.64      0.69       497\n",
      "weighted avg       0.90      0.91      0.89       497\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'alpha': [1, 10, 100, 1000]},]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    clf = GridSearchCV(RidgeClassifier(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando RandomizedSearchCV\n",
    "---\n",
    "\n",
    "Este método busca um conjunto aleatório de combinações possíveis de hiperparametros e retorna o modelo com o melhor score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanc\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2'}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='saga', tol=0.01, max_iter=200, random_state=0)\n",
    "\n",
    "distributions = dict(penalty=['l2', 'l1'])\n",
    "\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "\n",
    "search = clf.fit(X_test, y_test.values.ravel())\n",
    "\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação Cruzada\n",
    "---\n",
    "\n",
    "A escolha de qual técnica de *Machine Learning* utilizar não é algo fixo e pode variar de problema para problema. Escolher a melhor técnica e a melhor separação treino/teste é algo bastante subjetivo e que demanda muitas iterações. \n",
    "\n",
    "No caso do uso da ferramenta ```train_test_split```, deseja-se que tanto o conjunto de dados de treino quanto o de teste (validação) sejam aproveitados ao máximo. Isso significa que cada instância da base de dados que é *utilizada no* ou *retirada do* conjunto de teste *veio* ou *vai*, necessariamente, para o conjunto de treino. Isso pode implicar num resultado não tão ótimo. \n",
    "\n",
    "A validação cruzada é a forma de testar diversos métodos num mesmo dataset de treino/teste. Além disso, pode-se utilizar várias combinações de treino/teste, a fim de garantir que o resultado não tenha viés. Ao final, obtem-se a média dos resultados e esse se torna a representação principal do modelo de predição."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFolds\n",
    "\n",
    "\n",
    "Nesse tipo de classificação cruzada, primeiro os dados são divididos em k-subconjuntos treino/teste. O método irá, portanto, atribuir k-combinações diferentes para a separação treino/teste e, então, realizará treino e validação em cada uma das combinações. Ao final, os resultados serão uma média  de todos os k-resultados obtidos. A imagem abaixo ilustra o funcionamento de uma validação cruzada do tipo KFolds:\n",
    "\n",
    "![cv_explained](cv_explained.png \"Cross Validation Explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplificando KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4 0.5 0.6 0.7 0.8] [0.1 0.2]\n",
      "[0.1 0.2 0.5 0.6 0.7 0.8] [0.3 0.4]\n",
      "[0.1 0.2 0.3 0.4 0.7 0.8] [0.5 0.6]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.8] [0.7]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.7] [0.8]\n",
      "---\n",
      "[0.2 0.3 0.4 0.5 0.6 0.7 0.8] [0.1]\n",
      "[0.1 0.3 0.4 0.5 0.6 0.7 0.8] [0.2]\n",
      "[0.1 0.2 0.4 0.5 0.6 0.7 0.8] [0.3]\n",
      "[0.1 0.2 0.3 0.5 0.6 0.7 0.8] [0.4]\n",
      "[0.1 0.2 0.3 0.4 0.6 0.7 0.8] [0.5]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.7 0.8] [0.6]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.8] [0.7]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.7] [0.8]\n"
     ]
    }
   ],
   "source": [
    "# Amostra de dados\n",
    "data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "\n",
    "# Montando Folds\n",
    "kfold = KFold(5)\n",
    "\n",
    "# Mostrando a divisão feita em cada KFold\n",
    "for train, test in kfold.split(data):\n",
    "    #print(train , test)\n",
    "    print(data[train] , data[test])\n",
    "    \n",
    "# Exibindo a divisão com outro valor de k\n",
    "kfold = KFold(8)\n",
    "print(\"---\")\n",
    "for train, test in kfold.split(data):\n",
    "    #print(train , test)\n",
    "    print(data[train] , data[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando a acurácia usando *Cross Validation* e somente Classificação\n",
    "\n",
    "* **OBS:** ```cross_val_score``` aplica o método indicado como argumento em um número *k* de folds (especificado nos argumentos do método ```KFold()```) e obtém o score do fit de cada subconjunto. O método irá retornar os *scores* de cada iteração realizada por KFolds. A média desses valores será um resultado mais abrangente (próximo à realidade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia: 88.73%\n"
     ]
    }
   ],
   "source": [
    "# SGD Classifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(X_train, y_train.values.ravel())\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(\"Acuracia: %.2f%%\" % (sgd.score(X_test, y_test)*100.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.84751773 0.12765957 0.85409253 0.89679715 0.11032028 0.86120996\n",
      " 0.89323843 0.14590747 0.85409253 0.85765125]\n",
      "---\n",
      "Acuracia: 64.48%\n"
     ]
    }
   ],
   "source": [
    "# Utilizando a validação cruzada do tipo KFold\n",
    "kfold = KFold(n_splits=10)\n",
    "model_kfold = SGDClassifier()\n",
    "results_kfold = cross_val_score(model_kfold, X_train, y_train.values.ravel(), cv=kfold)\n",
    "\n",
    "print(\"Scores: \", results_kfold) \n",
    "\n",
    "print(\"---\\nAcuracia: %.2f%%\" % (results_kfold.mean()*100.0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold\n",
    "\n",
    "Neste método, cada conjunto comtém *aproximadamente* a mesma proporção de labels de destino que os dados completos. Isso é importante para que tanto no treino quanto no teste, o cenário seja o mais parecido com a realidade possível. \n",
    "\n",
    "Exemplo: considerando um dataset com transações de cartão de crédito em que se deseja predizer uma coluna \"possivel_fraude\", com valores 0 ou 1, suponha-se que haja 97% valores \"zero\" e 3% valores \"um\". Dessa forma, cada separação treino/teste será feita de forma que hajam apenas 3% de zeros no treino e no teste, aproximadamente, tornando mais parecido com a base de dados completa.\n",
    "\n",
    "A seguir, o método de validação cruzada do tipo *Stratified K-Fold* é aplicado à base de dados do Senai (senai_inep2.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.79787234 0.75886525 0.8683274  0.8683274  0.8683274  0.78291815\n",
      " 0.73309609 0.8683274  0.8683274  0.8683274 ]\n",
      "Accuracy: 82.83%\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=10)\n",
    "model_skfold = SGDClassifier()\n",
    "results_skfold = cross_val_score(model_skfold, X_train, y_train.values.ravel(), cv=skfold)\n",
    "print(\"Scores: \", results_skfold) \n",
    "print(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross-Validation (LOOCV)\n",
    "\n",
    "Esse tipo de validação cruzada é semelhante ao KFolds, só que elevado ao seu máximo nível de *k*. Ou seja, a cada iteração, o fold terá tamanho 1: o teste será apenas 1 fold e os demais (k-1) será para treino. Será, então, executado treino e validação para cada uma das possibilidades. Ao final, assim como em KFolds, os resultados serão uma média de todos os k-resultados obtidos. \n",
    "\n",
    "**Observações:** Essa variação é útil quando os dados de treinamento são de tamanho limitado e o número de parâmetros a serem testados não é alto. Do contrário, vai exigir muito processamento e demandar muito tempo (principalmente para treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [1. 1. 1. ... 1. 1. 1.]\n",
      "Accuracy: 76.53%\n"
     ]
    }
   ],
   "source": [
    "loocv = LeaveOneOut()\n",
    "model_loocv = SGDClassifier()\n",
    "results_loocv = cross_val_score(model_loocv, X_train, y_train.values.ravel(), cv=loocv)\n",
    "print(\"Scores: \", results_loocv) \n",
    "print(\"Accuracy: %.2f%%\" % (results_loocv.mean()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2812"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A título de curiosidade: número de testes realizados\n",
    "len(results_loocv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetição Aleatória de divisão entre treino e teste (Repeated Random Test-Train Splits)\n",
    "\n",
    "Essa técnica é uma mistura entre o tradicional ```train_test_split``` e o KFold (visto anteriormente). Primeiro, cria-se separações aleatórias de treino/teste dos dados, depois é feita sua avaliação (treino e validação). Esse mesmo processo de separação e execução dos dados de treino/teste é repetido multiplas vezes. No final, será retornado todos os valores de predição feito em cada *split* aleatório. A médias desses valores é utilizada para representar o resultado geral mais próximo à realidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.77132701 0.87559242 0.87085308 0.85900474 0.88270142 0.86848341\n",
      " 0.86255924 0.29620853 0.76895735 0.88270142]\n",
      "Accuracy: 79.38% (17.08%)\n"
     ]
    }
   ],
   "source": [
    "kfold2 = ShuffleSplit(n_splits=10, test_size=0.30)\n",
    "model_shufflecv = SGDClassifier()\n",
    "results_4 = cross_val_score(model_shufflecv, X_train, y_train.values.ravel(), cv=kfold2)\n",
    "print(\"Scores: \", results_4) \n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results_4.mean()*100.0, results_4.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinando Pipeline e Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 91.39%\n",
      "\n",
      "Best params:\n",
      " {'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__min_samples_leaf': 5, 'clf__min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#pipe_knn = Pipeline([('scl', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipetree = Pipeline([('scl', StandardScaler()), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "#pipe = [pipe_knn, pipetre]\n",
    "pipe = [pipetree]\n",
    "\n",
    "param_range = [1, 2, 3, 4, 5]\n",
    "\n",
    "# grid search params\n",
    "#grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "#               'clf__presort': [True, False]}]\n",
    "grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__min_samples_leaf': param_range,\n",
    "    'clf__max_depth': param_range,\n",
    "    'clf__min_samples_split': param_range[1:]}] \n",
    "\n",
    "    #'clf__presort': [True, False]}]  PARAMETRO DEFASADO\n",
    "\n",
    "# Construindo GridSearch\n",
    "gs = GridSearchCV(estimator=pipetree,\n",
    "                  param_grid=grid_params,\n",
    "                  scoring='accuracy')\n",
    "\n",
    "# Fit using grid search\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.2f%%' % (gs.best_score_*100.0))\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolhendo Features \n",
    "---\n",
    "\n",
    "* **Método *kbest***\n",
    "    * Utiliza método *chi-quadrado* para escolher melhor conjunto de features de acordo com o seguinte critério:\n",
    "        * Realiza um teste estatístico usando *chi-quadrado* entre cada features e classe\n",
    "        * O teste do *chi-quadrado* “elimina” features com maior probabilidade de serem independentes da classe e, portanto, irrelevantes para a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO_IES  NO_IES  CO_CURSO  NO_CURSO  TP_TURNO  TP_GRAU_ACADEMICO  \\\n",
      "0       1400       1     20516         1         3                  1   \n",
      "1       1400       1     20516         1         3                  1   \n",
      "2       1400       1     20516         1         3                  1   \n",
      "3       1400       1     20516         1         3                  1   \n",
      "4       1400       1     20516         1         3                  1   \n",
      "...      ...     ...       ...       ...       ...                ...   \n",
      "3304   14786       5   1332810         5         3                  1   \n",
      "3305   14786       5   1332810         5         3                  1   \n",
      "3306   15445       0   1258151         0         3                  3   \n",
      "3307   15445       0   1258151         0         3                  3   \n",
      "3308   15445       0   1258151         0         3                  3   \n",
      "\n",
      "      TP_MODALIDADE_ENSINO  ID_ALUNO  TP_COR_RACA  TP_SEXO  ...  \\\n",
      "0                        1      2519            1        1  ...   \n",
      "1                        1      1663            0        1  ...   \n",
      "2                        1      1537            2        2  ...   \n",
      "3                        1       957            0        2  ...   \n",
      "4                        1      1257            0        1  ...   \n",
      "...                    ...       ...          ...      ...  ...   \n",
      "3304                     1      3076            1        2  ...   \n",
      "3305                     1       172            1        2  ...   \n",
      "3306                     1      3204            1        2  ...   \n",
      "3307                     1      2739            1        1  ...   \n",
      "3308                     1       891            1        1  ...   \n",
      "\n",
      "      TP_NACIONALIDADE  IN_DEFICIENCIA  DT_INGRESSO_CURSO  IN_RESERVA_VAGAS  \\\n",
      "0                    1               0                  9                 0   \n",
      "1                    1               0                  9                 0   \n",
      "2                    1               0                 18                 0   \n",
      "3                    1               0                  2                 0   \n",
      "4                    1               0                 10                 0   \n",
      "...                ...             ...                ...               ...   \n",
      "3304                 1               0                 26                 0   \n",
      "3305                 1               0                 26                 0   \n",
      "3306                 1               0                 23                 0   \n",
      "3307                 1               0                 23                 0   \n",
      "3308                 1               0                 22                 0   \n",
      "\n",
      "      IN_FINANCIAMENTO_ESTUDANTIL  IN_APOIO_SOCIAL  \\\n",
      "0                               0                0   \n",
      "1                               0                0   \n",
      "2                               0                0   \n",
      "3                               0                0   \n",
      "4                               0                0   \n",
      "...                           ...              ...   \n",
      "3304                            0                0   \n",
      "3305                            0                0   \n",
      "3306                            0                0   \n",
      "3307                            0                0   \n",
      "3308                            0                0   \n",
      "\n",
      "      IN_ATIVIDADE_EXTRACURRICULAR  TP_ESCOLA_CONCLUSAO_ENS_MEDIO  \\\n",
      "0                                0                              1   \n",
      "1                                0                              1   \n",
      "2                                0                              1   \n",
      "3                                0                              9   \n",
      "4                                0                              1   \n",
      "...                            ...                            ...   \n",
      "3304                             0                              1   \n",
      "3305                             0                              1   \n",
      "3306                             0                              1   \n",
      "3307                             0                              1   \n",
      "3308                             0                              1   \n",
      "\n",
      "      IN_MATRICULA  IN_CONCLUINTE  \n",
      "0                0              0  \n",
      "1                1              1  \n",
      "2                1              1  \n",
      "3                0              0  \n",
      "4                0              0  \n",
      "...            ...            ...  \n",
      "3304             1              0  \n",
      "3305             1              0  \n",
      "3306             1              1  \n",
      "3307             1              1  \n",
      "3308             0              0  \n",
      "\n",
      "[3309 rows x 21 columns]\n",
      "[[   1400   20516       0]\n",
      " [   1400   20516       1]\n",
      " [   1400   20516       1]\n",
      " ...\n",
      " [  15445 1258151       1]\n",
      " [  15445 1258151       1]\n",
      " [  15445 1258151       0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "X_new = SelectKBest(chi2, k=3).fit_transform(X, y)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conforme visto acima, apenas os atributos ```CO_IES``` e ```CO_CURSO``` são os mais relevantes para predizer ```TP_SITUACAO```, segundo o resultado da aplicação do método *kBest*.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
